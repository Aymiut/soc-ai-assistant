# üõ°Ô∏è SOC AI Assistant

**Assistant intelligent pour la d√©tection et l'analyse d'attaques cybern√©tiques autonomes propuls√©es par l'IA**

## üìã Vue d'ensemble du projet

Ce projet d√©veloppe un syst√®me d'assistance pour Security Operations Center (SOC) capable de d√©tecter et analyser les cyberattaques orchestr√©es par l'intelligence artificielle, directement inspir√© du rapport Anthropic de novembre 2025 sur la premi√®re campagne de cyber-espionnage orchestr√©e par IA (GTG-1002).

### Contexte

Le rapport Anthropic r√©v√®le qu'un groupe sponsoris√© par un √âtat a utilis√© Claude Code pour mener des attaques o√π l'IA ex√©cutait **80-90% des op√©rations tactiques de mani√®re autonome** :
- Reconnaissance automatis√©e des infrastructures
- D√©couverte et exploitation de vuln√©rabilit√©s
- Mouvement lat√©ral dans les r√©seaux
- Extraction et analyse de donn√©es sensibles
- Rythme d'attaque surhumain (plusieurs op√©rations par seconde)

### Objectif

D√©velopper un syst√®me d√©fensif qui :
1. **D√©tecte** les patterns d'attaques autonomes pilot√©es par IA
2. **Analyse** les alertes de s√©curit√© avec contexte MITRE ATT&CK
3. **Recommande** des actions de r√©ponse aux analystes SOC
4. **Automatise** le triage des alertes pour r√©duire la charge cognitive

## üéØ Phases du projet

### Phase 1 : MVP avec alertes simul√©es (EN COURS - 95% COMPL√âT√â)
- ‚úÖ Analyse d'alertes JSON simul√©es
- ‚úÖ Int√©gration Ollama (Llama 3.2) en local
- ‚úÖ Construction de prompts structur√©s pour analyse SOC
- ‚úÖ Communication avec API Ollama (POST requests)
- ‚úÖ Gestion d'erreurs robuste avec try/except
- ‚úÖ Analyse en batch d'alertes multiples avec enumerate()
- ‚úÖ Fonction main() compl√®te pour orchestration
- ‚úÖ Chargement JSON avec gestion d'erreurs
- ‚úÖ Affichage de progression [x/total]
- ‚úÖ Sauvegarde automatique des r√©sultats dans logs/
- ‚úÖ Statistiques de synth√®se par s√©v√©rit√©
- üîÑ D√©tection de patterns d'attaque IA (en cours d'affinement)
- üîÑ Recommandations bas√©es sur MITRE ATT&CK (en cours d'affinement)
- üéØ **Co√ªt : 0‚Ç¨** (100% local)

### Phase 2 : Int√©gration IDS r√©el
- üîÑ D√©ploiement Suricata (IDS open-source)
- üîÑ Ingestion temps r√©el des alertes
- üîÑ Corr√©lation d'√©v√©nements
- üîÑ Interface web Flask
- üéØ **Co√ªt : 0-5‚Ç¨/mois** (VPS optionnel)

### Phase 3 : SOC Automation avanc√©
- ‚è≥ Orchestration de r√©ponses avec Ansible
- ‚è≥ Playbooks automatis√©s
- ‚è≥ Dashboard Grafana
- ‚è≥ API publique
- üéØ **Co√ªt : 10-30‚Ç¨/mois**

## üóÇÔ∏è Architecture technique

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Phase 1 (MVP)                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                         ‚îÇ
‚îÇ  [Alertes JSON: data/sample_alerts.json]               ‚îÇ
‚îÇ           ‚Üì                                             ‚îÇ
‚îÇ  [main() - Orchestration]                              ‚îÇ
‚îÇ           ‚Üì                                             ‚îÇ
‚îÇ  [load_alerts_from_file() - Chargement s√©curis√©]      ‚îÇ
‚îÇ           ‚Üì                                             ‚îÇ
‚îÇ  [analyze_batch() - Boucle avec enumerate()]           ‚îÇ
‚îÇ           ‚Üì                                             ‚îÇ
‚îÇ  [build_prompt() - Prompt adaptatif]                   ‚îÇ
‚îÇ           ‚Üì                                             ‚îÇ
‚îÇ  [send_to_ollama() - API Ollama ‚Üí Llama 3.2 local]     ‚îÇ
‚îÇ           ‚Üì                                             ‚îÇ
‚îÇ  [Analyse + Classification MITRE + Recommandations]    ‚îÇ
‚îÇ           ‚Üì                                             ‚îÇ
‚îÇ  [save_results() - Export JSON horodat√©]               ‚îÇ
‚îÇ           ‚Üì                                             ‚îÇ
‚îÇ  [Affichage CLI + Statistiques par s√©v√©rit√©]          ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Stack technique

| Composant | Technologie | Justification |
|-----------|-------------|---------------|
| **LLM** | Ollama + Llama 3.2 | Local, gratuit, optimis√© Apple Silicon |
| **Langage** | Python 3.10+ | √âcosyst√®me ML/Sec robuste |
| **IDS** (Phase 2) | Suricata | Open-source, performant |
| **Conteneurisation** | Docker | Isolation et reproductibilit√© |
| **Orchestration** (Phase 3) | Ansible | Standard industrie |
| **Visualisation** (Phase 3) | Grafana | Dashboards SOC |

## üîß Setup de l'environnement

### Pr√©requis

- **OS** : macOS (Apple Silicon M1/M2/M3) ou Linux
- **RAM** : 16 GB minimum (recommand√© pour Llama 3.2)
- **Disk** : 10 GB libres
- **Docker Desktop** : Install√© et fonctionnel
- **Python** : 3.10 ou sup√©rieur

### Installation compl√®te

#### 1. Installation d'Ollama

```bash
# Via Homebrew (macOS)
brew install ollama

# V√©rification
ollama --version

# Lancement du serveur Ollama (garder ce terminal ouvert)
ollama serve
```

#### 2. T√©l√©chargement du mod√®le Llama 3.2

Dans un **nouveau terminal** :

```bash
# T√©l√©charge Llama 3.2 (environ 2 GB)
ollama pull llama3.2

# V√©rification
ollama list

# Test rapide
ollama run llama3.2 "Explique ce qu'est une attaque par force brute SSH"
```

#### 3. Clone et setup du projet

```bash
# Cr√©ation de la structure
mkdir ~/soc-ai-assistant
cd ~/soc-ai-assistant

# Structure des dossiers
mkdir -p data logs scripts config

# Fichiers principaux
touch scripts/analyzer.py
touch data/sample_alerts.json
```

#### 4. Configuration Python

```bash
# Cr√©ation environnement virtuel
python3 -m venv venv
source venv/bin/activate  # Sur macOS/Linux

# Installation des d√©pendances
pip install requests

# V√©rification
python --version  # Doit afficher Python 3.10+
```

#### 5. Ajout des fichiers du projet

Copier les fichiers suivants dans leurs emplacements respectifs :
- `data/sample_alerts.json` ‚Üí Alertes simul√©es (8 sc√©narios d'attaque)
- `scripts/analyzer.py` ‚Üí Script principal d'analyse

**Structure actuelle du code** :
```python
# Fonctions principales impl√©ment√©es :
build_prompt(alert)              # Construit un prompt adaptatif avec .get()
send_to_ollama(prompt)           # Envoie le prompt √† l'API Ollama locale
load_alerts_from_file(path)      # Charge les alertes avec gestion d'erreurs
analyze_batch(alerts)            # Analyse multiple avec enumerate(start=1)
save_results(results, path)      # Sauvegarde JSON horodat√©e
main()                           # Orchestration compl√®te + statistiques
```

#### 6. Lancement du syst√®me

```bash
# Terminal 1 : Ollama doit tourner
ollama serve

# Terminal 2 : Lancement de l'analyse
cd ~/soc-ai-assistant
source venv/bin/activate
python scripts/analyzer.py
```

### Commandes utiles

```bash
# Activer l'environnement Python
source venv/bin/activate

# D√©sactiver l'environnement
deactivate

# V√©rifier les mod√®les Ollama disponibles
ollama list

# Supprimer un mod√®le (lib√©rer espace)
ollama rm llama3.2

# Logs Ollama
tail -f ~/.ollama/logs/server.log

# Analyser toutes les alertes (main() le fait par d√©faut)
python scripts/analyzer.py

# Voir les r√©sultats sauvegard√©s
cat logs/analysis_results.json | python -m json.tool
```

## ü§î Pourquoi ex√©cuter le LLM en local ?

### Avantages techniques

1. **Co√ªt 0‚Ç¨**
   - Pas de frais d'API (Claude API : ~0,003‚Ç¨/1K tokens)
   - Exp√©rimentation illimit√©e pendant le d√©veloppement
   - Pas de surprise de facturation

2. **Confidentialit√© et s√©curit√©**
   - Les alertes de s√©curit√© ne quittent **jamais** ta machine
   - Aucune d√©pendance √† des services cloud tiers
   - Conformit√© RGPD native
   - Critique pour manipuler des vraies alertes sensibles

3. **Latence optimis√©e**
   - Pas de round-trip r√©seau
   - Sur Mac M2 : ~1-3 secondes par analyse
   - Important pour traiter des centaines d'alertes/heure

4. **Contr√¥le total**
   - Pas de rate limiting
   - Choix du mod√®le (Llama, Mistral, etc.)
   - Customisation des param√®tres (temp√©rature, context window)

5. **Offline-first**
   - Fonctionne sans connexion Internet
   - R√©silience face aux pannes cloud
   - Id√©al pour environnements isol√©s (air-gapped)

### Limites connues

| Crit√®re | Ollama local | Claude API |
|---------|--------------|------------|
| **Performance** | Bien (Llama 3.2 8B) | Excellent (Sonnet 4) |
| **Co√ªt** | 0‚Ç¨ | ~10-30‚Ç¨/mois |
| **Raisonnement complexe** | Bon | Sup√©rieur |
| **Vitesse** | 1-3 sec | 0.5-1 sec |
| **Setup** | Configuration requise | Imm√©diat |

### Quand migrer vers une API cloud ?

‚úÖ **Reste en local si** :
- Tu d√©veloppes/apprends
- Tu traites des donn√©es sensibles
- Budget limit√©
- Environnement air-gapped

üîÑ **Consid√®re l'API si** :
- Tu passes en production √† grande √©chelle
- Tu as besoin de raisonnement tr√®s avanc√©
- Tu veux d√©l√©guer l'infrastructure
- Budget disponible (>50‚Ç¨/mois)

### Performances sur Mac M2

Benchmarks r√©els avec Llama 3.2 (8B) :

```
Analyse d'une alerte simple : ~2 secondes
Analyse d'une alerte complexe : ~4 secondes
Batch de 10 alertes : ~25 secondes
Throughput : ~24 alertes/minute
```

**Conclusion** : Largement suffisant pour un SOC de taille moyenne (< 1000 alertes/jour).

## üìä Donn√©es de test

Le fichier `data/sample_alerts.json` contient 8 alertes simulant une attaque compl√®te inspir√©e du rapport Anthropic :

1. **Port Scan** (47 ports/sec) ‚Üí Pattern IA
2. **SSH Brute Force** (156 tentatives/2min) ‚Üí Pattern IA
3. **Lateral Movement** ‚Üí Post-exploitation
4. **Data Exfiltration** (2.3 GB) ‚Üí Objectif atteint
5. **SQL Injection** ‚Üí Vecteur d'attaque web
6. **Command Execution** (12 cmd/min) ‚Üí Pattern IA
7. **Anomalous API Activity** (45 req/sec) ‚Üí Pattern IA
8. **Credential Harvesting** ‚Üí Persistence

Ces alertes couvrent les principales phases du framework MITRE ATT&CK.

## üìù Exemple de sortie

```
============================================================
üõ°Ô∏è  SOC AI ASSISTANT - ANALYSEUR D'ALERTES
============================================================

üìÇ Chargement des alertes depuis data/sample_alerts.json...
‚úÖ 8 alertes charg√©es avec succ√®s

üîç D√©marrage de l'analyse de 8 alertes...

üìä Analyse de l'alerte 1/8: alert_001
------------------------------------------------------------

‚úÖ ANALYSE:
CRITICIT√â: CRITICAL

TACTIQUE MITRE ATT&CK: 
- T1046 - Network Service Discovery (Reconnaissance)
- T1595 - Active Scanning

INDICATEURS D'ATTAQUE AUTONOME PAR IA: OUI
- Rythme anormal: 47 ports scann√©s par seconde
- Vitesse surhumaine incompatible avec scan manuel
- Pattern syst√©matique typique d'automatisation IA

RECOMMANDATIONS D'ACTION IMM√âDIATE:
1. Bloquer imm√©diatement l'IP source au firewall p√©rim√©trique
2. V√©rifier les logs d'acc√®s pour tentatives d'exploitation
3. Activer l'IPS en mode pr√©vention sur ce segment
4. Alerter l'√©quipe IR pour investigation approfondie
5. Examiner le trafic r√©seau pour d'autres sources similaires

FAUX POSITIF PROBABLE: NON
Haute confiance - Pattern d'attaque confirm√©

============================================================

üíæ R√©sultats sauvegard√©s dans logs/analysis_results.json

============================================================
üìà STATISTIQUES
============================================================
Total alertes analys√©es: 8
  - high: 3
  - critical: 4
  - medium: 1
============================================================
```

## üéì Progression du d√©veloppement

### ‚úÖ √âtapes compl√©t√©es

**Semaine 1 : Fondations**
- [x] Setup environnement Ollama + Llama 3.2
- [x] Compr√©hension de l'architecture SOC
- [x] Premiers tests d'interaction avec API LLM
- [x] Construction de prompts structur√©s
- [x] Fonction `build_prompt()` avec contexte MITRE ATT&CK
- [x] Fonction `send_to_ollama()` avec gestion d'erreurs basique
- [x] Tests sur alerte unique

**Semaine 2 : Analyse batch et orchestration**
- [x] Fonction `build_prompt()` adaptative avec `.get()`
- [x] Support multi-structure (alert_type vs type)
- [x] Fonction `load_alerts_from_file()` avec gestion d'erreurs
- [x] Fonction `analyze_batch()` avec `enumerate(start=1)`
- [x] Affichage de progression visuel [x/total]
- [x] Fonction `save_results()` avec export JSON horodat√©
- [x] Fonction `main()` compl√®te pour orchestration
- [x] Statistiques de synth√®se par s√©v√©rit√©
- [x] Gestion robuste des erreurs (FileNotFoundError, JSONDecodeError)
- [x] Tests r√©ussis sur les 8 alertes simul√©es

### üîÑ En cours d'am√©lioration

**Phase actuelle : Optimisation des analyses**
- [ ] Affiner la d√©tection des patterns IA (scoring quantitatif)
- [ ] Enrichir les recommandations MITRE ATT&CK
- [ ] Ajouter scoring de confiance (0-100%)
- [ ] Impl√©menter filtrage par s√©v√©rit√©
- [ ] Ajouter m√©triques de performance (temps/alerte)

### ‚è≥ Prochaines √©tapes

**Court terme (prochaine session)**
- [ ] Export rapport en HTML/Markdown
- [ ] Graphiques de distribution (matplotlib)
- [ ] D√©tection de corr√©lation temporelle
- [ ] Mode verbose/quiet configurable
- [ ] Tests unitaires avec pytest

## üöÄ Roadmap d√©taill√©e

### Court terme (1-2 semaines)
- [ ] Scoring de confiance quantitatif
- [ ] Filtrage par criticit√©/s√©v√©rit√©
- [ ] Export rapport HTML avec styling
- [ ] M√©triques de performance d√©taill√©es
- [ ] Mode debug pour troubleshooting

### Moyen terme (1 mois)
- [ ] Int√©gration Suricata (IDS r√©el)
- [ ] Base de donn√©es SQLite pour historique
- [ ] Interface web Flask basique
- [ ] Corr√©lation temporelle d'alertes
- [ ] API REST pour int√©gration SIEM

### Long terme (3 mois)
- [ ] Dashboard Grafana temps r√©el
- [ ] Playbooks Ansible automatis√©s
- [ ] Machine Learning pour false positive reduction
- [ ] Documentation compl√®te API
- [ ] Conteneurisation Docker compl√®te

## üìñ Journal d'apprentissage

### Session 1 - Fondations (18 Nov 2025)

**Objectifs** : Comprendre l'interaction avec un LLM et construire les bases du syst√®me

**Concepts ma√Ætris√©s** :
1. **API REST avec Ollama** : Communication via `requests.post()` avec corps JSON
2. **Prompt Engineering** : Structure en 5 parties (R√¥le, Contexte, Donn√©es, Instructions, Format)
3. **Streaming vs Non-streaming** : Choix du mode non-streaming pour analyses structur√©es
4. **Gestion d'erreurs** : Try/except pour robustesse du syst√®me

**Code d√©velopp√©** :
- `build_prompt(alert)` : Construction de prompts contextualis√©s pour analyse SOC
- `send_to_ollama(prompt)` : Communication avec l'API locale Ollama
- Tests r√©ussis sur alerte SSH Brute Force simul√©e

### Session 2 - Orchestration et robustesse (29 Nov 2025)

**Objectifs** : Construire un syst√®me complet d'analyse batch avec gestion d'erreurs

**Concepts ma√Ætris√©s** :
1. **M√©thode `.get()` pour dictionnaires** : Acc√®s s√©curis√© avec valeurs par d√©faut
2. **`enumerate(start=1)`** : Compteur lisible dans les boucles
3. **Gestion d'erreurs JSON** : FileNotFoundError, JSONDecodeError
4. **Architecture modulaire** : S√©paration des responsabilit√©s (load/analyze/save/main)
5. **Timestamps ISO 8601** : Horodatage standardis√© avec `datetime.now().isoformat()`

**Code d√©velopp√©** :
- `build_prompt()` adaptatif : Support multi-structure avec `.get()`
- `load_alerts_from_file()` : Chargement robuste avec gestion d'erreurs
- `analyze_batch()` : Boucle optimis√©e avec `enumerate()`
- `save_results()` : Export JSON avec m√©tadonn√©es
- `main()` : Orchestration compl√®te du flux d'analyse

**Probl√®me r√©solu** :
- **KeyError: 'type'** ‚Üí Solution : `.get("alert_type", alert.get("type", "Unknown"))`
- Adaptation du script √† la structure r√©elle du JSON
- Correction du chemin : `logs/` ‚Üí `data/sample_alerts.json`

**Tests r√©ussis** :
- Analyse compl√®te des 8 alertes simul√©es
- Sauvegarde automatique dans `logs/analysis_results.json`
- Statistiques de synth√®se par s√©v√©rit√©

**Apprentissages cl√©s** :
- Toujours utiliser `.get()` pour acc√©der aux cl√©s de dictionnaires incertaines
- `enumerate()` est plus pythonique que `list.index()`
- Une fonction `main()` claire facilite l'orchestration et les tests
- Les statistiques finales ajoutent de la valeur √† l'analyse

**M√©triques** :
- Temps moyen par alerte : ~3 secondes
- Throughput : ~20 alertes/minute
- Taux de r√©ussite : 100% (8/8 alertes analys√©es)

---

## üìö Ressources

### S√©curit√© offensive et d√©fensive
- [MITRE ATT&CK Framework](https://attack.mitre.org/)
- [Suricata Documentation](https://suricata.io/documentation/)
- [TryHackMe - SOC Level 1](https://tryhackme.com/path/outline/soclevel1)

### IA et LLMs
- [Ollama Documentation](https://github.com/ollama/ollama)
- [Llama 3.2 Model Card](https://ai.meta.com/llama/)
- [Anthropic - Threat Intelligence](https://www.anthropic.com/research)

### Python Best Practices
- [Python Requests Library](https://requests.readthedocs.io/)
- [JSON Handling in Python](https://docs.python.org/3/library/json.html)
- [Python Error Handling](https://docs.python.org/3/tutorial/errors.html)

### Rapport de r√©f√©rence
- [Anthropic - First AI-Orchestrated Cyber Espionage Campaign (Nov 2025)](https://www.anthropic.com/research)

## ü§ù Contribution

Ce projet est open-source et √©ducatif. Les contributions sont les bienvenues :
- üêõ Signaler des bugs
- üí° Proposer des am√©liorations
- üìñ Am√©liorer la documentation
- üîß Soumettre des PR

## ‚öñÔ∏è Licence

MIT License - Utilisation libre √† des fins √©ducatives et de recherche en s√©curit√©.

**‚ö†Ô∏è Avertissement** : Ce projet est destin√© uniquement √† des fins √©ducatives et de recherche en cybers√©curit√© d√©fensive. L'utilisation de ces techniques √† des fins malveillantes est ill√©gale.

## üìß Contact

Pour questions ou suggestions : Ouvrir une issue sur GitHub

---

**Version** : 1.1.0-MVP (stable)  
**Derni√®re mise √† jour** : 29 Novembre 2025  
**Statut** : üü¢ Phase 1 compl√©t√©e √† 95% - Syst√®me d'analyse batch fonctionnel  
**Prochaine session** : Optimisation des analyses et scoring de confiance